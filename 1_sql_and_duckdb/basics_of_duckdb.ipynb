{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5027519-220f-40d0-8f0e-d7ae089c3b0e",
   "metadata": {},
   "source": [
    "# Introduction to DuckDB\n",
    "\n",
    "## Why SQL?  Isn't it ancient technology?\n",
    "\n",
    "Since the Cambrian-like language explosion of recent years, it's worth wondering why we would even use a technology invented in the early 1970s (although LISP fans won't have such concerns for obvious reasons).  The short answer is that it's still around as ancient technology because it works; the longer answer is that it provides an excellent layer of abstraction.  If I can query a data source reliable with SQL, I don't care about what data source it is, and these \"cut planes\" allow sections of a codebase to evolve independently.\n",
    "\n",
    "## Dude, where's my python?\n",
    "\n",
    "It's coming.  I wanted to start with something language-agnostic.  For the moment, python is the language of choice for AI-related projects, but it is not the only one; as DuckDB and SQL can be used by different languages, I felt it important to show the capabilities outside of a standard programming language.\n",
    "\n",
    "## Attributions\n",
    "\n",
    "Much of the basic structure of this notebook comes from the JupySQL duckdb integration page (https://jupysql.ploomber.io/en/latest/integrations/duckdb.html) and other pages, rather lightly adapted because, frankly, I was short on time.  We'll try to use more relevant data in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4430beb-aaa5-41c4-a46c-b8d5e7e470e9",
   "metadata": {},
   "source": [
    "# Your Workshop Needs More TLC\n",
    "\n",
    "We'll start out by analyzing some data locally from the New York City Taxi and Limousine Commission (https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).  This is one of many nice free datasets out there; you can retrieve a bit of it locally with the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c475aab0-3072-4569-b538-17b8b9fa702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "N_MONTHS = 3\n",
    "\n",
    "# https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "for i in range(1, N_MONTHS + 1):\n",
    "    filename = f\"yellow_tripdata_2021-{str(i).zfill(2)}.parquet\"\n",
    "    if not Path(filename).is_file():\n",
    "        print(f\"Downloading: {filename}\")\n",
    "        url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{filename}\"\n",
    "        urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b17e5e1-97f4-4ea1-870a-1dab8ca82846",
   "metadata": {},
   "source": [
    "These are only three parquet files, because we're only looking at three months, and they look pretty innocuous (22M-30M each).  But parquet can be deceiving.  How much would each one take up materialized in memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39b9848-b40d-49f0-a02f-c80434c2b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"yellow_tripdata_2021-01.parquet\")\n",
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de58887e-2b4e-4314-9413-1c5e6346c2a0",
   "metadata": {},
   "source": [
    "\n",
    "So, rough order of magnitude this looks like about a 10x compression, and this is clearly a toy dataset for our purposes; if each month only materialized to 250Mb we could get about ten years of data into memory on a 32Gb machine without hitting virtual memory, but it's still worth playing with.  Let's see what we can learn without invoking more Python (directly).\n",
    "First we'll create a table in our in-memory duckdb database.  In a regular database, this represents data stored within the database, and it does here too--it's just that the raw data is retrieved from the parquet files we provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe16a7e-9166-42fc-89b5-1c493791d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%sql duckdb://"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61939ff6-0298-4ab0-b99a-b0261f78d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE TABLE\n",
    "  tlc AS\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  read_parquet ([\"yellow*parquet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b2442f-821a-40e8-8414-a8998d11883c",
   "metadata": {},
   "source": [
    "# Wut\n",
    "\n",
    "If you haven't used SQL before, this looks a bit arcane; SQL is irritating in some ways because it relies on some convention (conventionally keywords are ALL CAPS but it's not necessary), mostly ignores whitespace, and can be oddly picky about quote marks and names.\n",
    "\n",
    "It can also be quite dangerous in production.  A deep knowledge of SQL is a handy thing, and while LLMs can do a great job of basic stuff, the implications of some choices can be... less predictable.\n",
    "\n",
    "This scrap of code does nothing more than create a \"table\" in our database consisting of all the data in all the parquet files we give it.  I've done this one as `CREATE OR REPLACE` so that the cell in the notebook can be re-run easily, but just `CREATE` would have worked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754574c4-bcc8-4062-9afe-9eca44005732",
   "metadata": {},
   "source": [
    "# Querying our \"Database\"\n",
    "\n",
    "We now have a virtual database, consisting of one table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91a5a0d-56e5-4f2d-a2a4-516592c08e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sqlcmd tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6805b77-8ae0-492f-ab18-be1bf6b26ce4",
   "metadata": {},
   "source": [
    "# sqlcmd?\n",
    "\n",
    "The \"sql\" and \"sqlcmd\" magic commands in jupysql do slightly different things.  `%%sql` executes an SQL query as if it were sent to the database; the `%sqlcmd` acts as if we were administrating the database itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6149515d-a891-487f-83ff-59a272d7e98f",
   "metadata": {},
   "source": [
    "...and we can thus examine the database as if it were an SQL-compatible database, even though we just have rough parquet files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d490a7-58ec-4c71-a5db-98d2f62fee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sqlcmd columns -t tlc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265b6f2d-3167-44af-bb18-5956ebb9da35",
   "metadata": {},
   "source": [
    "And, of course, query it using SQL just as if it were a database; we'll select three columns of five different rows to inspect some sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18441f94-2d0d-499b-b2b8-1e526e8da1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count\n",
    "FROM tlc\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59318d5d-0e31-42ac-b53b-4c465bbba562",
   "metadata": {},
   "source": [
    "\n",
    "# Note\n",
    "\n",
    "`LIMIT 5` does just what it says--limits the results to 5 rows.  Using `LIMIT` when exploring data is generally a very good idea; it makes the queries smaller and prevents your visual buffer from overflowing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd26806-ebd6-4676-9cc1-6a3105a5a76e",
   "metadata": {},
   "source": [
    "So far, none of this looks particularly impressive.  What's interesting, however, is that we've interacted with our database in a unified manner without actually materializing any of the data into an in-memory table.\n",
    "\n",
    "And that can be important, particularly for aggregate data.  The following query aggregates our TLC data by passenger count, getting us the number of trips by number of passengers, and the average length of those trips.  Doing this in pandas would require materialization of the whole dataframe and additional resources for the aggregation; using duckdb, we can get that information with very little code, and add further data sources as necessary.\n",
    "\n",
    "The query is also quite efficient, as duckdb optimizes reads to the column-oriented parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82388eb3-9b28-47bf-98d8-a1b782aab972",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "    passenger_count, COUNT(*) as num_trips, AVG(trip_distance) AS avg_trip_distance\n",
    "FROM \"yellow_tripdata_2021-01.parquet\"\n",
    "GROUP BY passenger_count\n",
    "ORDER BY passenger_count ASC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a82a8-c38a-4382-b944-094ea251b465",
   "metadata": {},
   "source": [
    "JupySQL's interface with duckdb doesn't just allow some transforms like this with memory efficiency--it also allows you to *plot* data with high efficiency.  Let's look at this with two different new concepts.\n",
    "\n",
    "First we'll store an SQL query without actually executing the query, to give ourselves something to pull from later.  Second, we'll plot a histogram of trip distance, again without materializing the whole table at any point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cfce4e-6cc1-4791-8504-4216b96d1046",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql --save histogram_base --no-execute\n",
    "SELECT trip_distance\n",
    "FROM tlc\n",
    "WHERE trip_distance < 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e06cf75-8adf-4714-b4b7-c5af33abebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sqlplot histogram --table histogram_base --column trip_distance --bins 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d791bc0a-f12e-4884-abf5-b028c684f29d",
   "metadata": {},
   "source": [
    "# Simple ETL/ELT\n",
    "\n",
    "One of the most basic parts of any data pipeline is Extract/Transform/Load (although recently we're seeing variations like Extract/Load/Transform, where raw data is dumped into a \"data lake\" and transformed via SQL-style queries to produce data sets suitable for ML training)\n",
    "\n",
    "DuckDB plays well with many ETL-related tools such as Dagster, Airflow, DBT, and more.  Here, let's look at a simple straightforward format for a data \"pond\" which covers a lot of use cases and scales very well up to a point: parquet files stored in hive partitions.\n",
    "\n",
    "A single parquet file is easily queriable, as is many (hundreds).  But as the number of files goes up, the query performance goes down.  If you know what columns will form natural partitions (stock market queries by symbol, taxi rides by number of passengers, etc) you can choose a directory structure that encodes the column values in the directory names; this is called \"hive\" partitioning.\n",
    "\n",
    "We can even do this transformation without any creation of tables, such as below, where we select a subset of data (vendor id, passenger count, trip distance, and total cost) and save to a set of parquet files partitioned by vendor id and passenger count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e939c-7f5c-42db-8359-fd7b5b4104df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "COPY (SELECT\n",
    "    VendorID, passenger_count, trip_distance, payment_type\n",
    "    FROM read_parquet([\"yellow*parquet\"]))\n",
    "    TO 'example_hive'\n",
    "    (FORMAT 'parquet', COMPRESSION 'zstd',\n",
    "    PARTITION_BY (VendorID, passenger_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cd5059-0474-46aa-b4c5-ece9a4eea197",
   "metadata": {},
   "source": [
    "What did this do?  We can investigate the result with a bit of python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3f7d1e-4022-4e98-b288-aec562be9b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def print_directory_contents(path, indent=\"\"):\n",
    "    p = Path(path)\n",
    "    for item in p.iterdir():\n",
    "        print(f\"{indent}{item.name} ({item.stat().st_size} bytes)\")\n",
    "        if item.is_dir():\n",
    "            print_directory_contents(item, indent + \"  \")\n",
    "\n",
    "print_directory_contents(\"./example_hive\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59a48ec-85bd-4e30-b47f-f5bebafd8e20",
   "metadata": {},
   "source": [
    "# Part 1 Conclusion\n",
    "\n",
    "Nice!  You've learned about scanning external sources (parquet in this case, but CSV works too), querying via SQL, and transforming to hive-partitioned parquet.\n",
    "\n",
    "Let's move on to using Python, the duckdb library, and \"real\" data stored in S3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
