{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7cf75b9-15f7-4000-9b7a-2686314079d4",
   "metadata": {},
   "source": [
    "# ETL With DuckDB part 2: Basic ETL\n",
    "\n",
    "We'd now like to proceed to a somewhat real task: querying a decent amount of data in a way that won't materialize it into memory and converting it into a local hive which we can query at our leisure.  Let's start by creating an in-memory duckdb connection which represents our sources of data as one table as before we'll make our in-memory connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d740d72-c9f4-4552-8413-4304334bb2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "def create_connection():\n",
    "    # let's set up our duckdb in memory connection\n",
    "    conn=duckdb.connect(\":memory:\")\n",
    "    # the in-memory connection now must be extended to support https access to S3\n",
    "    conn.install_extension(\"httpfs\")\n",
    "    conn.load_extension(\"httpfs\")\n",
    "    # we'll also set up credentials.  Doing it this way is NOT recommended; never\n",
    "    # store secrets in repositories in production!\n",
    "    conn.execute(\"\"\"\n",
    "    CREATE SECRET secret (\n",
    "      TYPE S3,\n",
    "      KEY_ID 'DO801T8KVC4GP7XCU74A',\n",
    "      SECRET 'lZVY1vZlGUYJRim+f1WRpVYmv7PtJvYffheKSW4iJOQ',\n",
    "      REGION 'US',\n",
    "      ENDPOINT 'lon1.digitaloceanspaces.com'\n",
    "    )\"\"\")\n",
    "    return conn\n",
    "\n",
    "conn = create_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0cd80e-abee-4cb5-8390-3338d09be7ea",
   "metadata": {},
   "source": [
    "But this time we'll build up our data source from multiple parquet files (duckdb supports wildcards as well and as we'll see later can treat a hive partition in a single read, but for now we'll assemble these as if they represent a single data source--another benefit of duckdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ddd94-eecf-4fc9-9336-edf681e07cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = [\"CSCO\", \"LSTR\", \"NFLX\", \"SHLS\", \"SOFI\", \"WING\"]\n",
    "files = [f\"s3://thingotron-qs1/artifacts/databento_1_{symbol}_parquet:latest/{symbol}.parquet\" for symbol in symbols]\n",
    "\n",
    "rel=conn.sql(f\"\"\"SELECT * FROM read_parquet([{\", \".join([\"'\"+file+\"'\" for file in files])}])\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a04d05-0b2f-4d3f-bd3d-839c24331c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bbbfbd-61e9-4e74-9489-d61cfd857ad0",
   "metadata": {},
   "source": [
    "## Check: How would you list the columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c79073-9e8e-4d23-bda6-327de3377c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rel.query(\"rel\", \"DESCRIBE rel\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c826c4a-04af-4218-a4d4-02c47935d25b",
   "metadata": {},
   "source": [
    "Now, the databento folks use a sentinel value (9223372036854775807) to mark price values where the price isn't set.  We'll get a slightly smaller dataset by restricting the query a bit for the next step by only selecting those rows with level 0 defined on both sides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf03b2d2-0fc6-410c-aa2f-6157e24d481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel2 = rel.query(\"rel\", \"SELECT * FROM rel WHERE buyside_price_00 < 9.2e13 AND sellside_price_00 < 9.2e13\")\n",
    "print(rel2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aa64ad-fd56-406c-b824-ee705a516ad9",
   "metadata": {},
   "source": [
    "# The ETL step\n",
    "\n",
    "Suppose we don't care about much of the data in there and want to write an ETL step which takes only the timestamp, symbol, and first level of the Limit Order Book, represented by the following columns:\n",
    "\n",
    "symbol\n",
    "time_stamp\n",
    "buyside_price_00\n",
    "sellside_price_00\n",
    "\n",
    "Write some code now that \n",
    "* takes the above relation `rel`, \n",
    "* selects the above columns from it, \n",
    "* Converts the prices by dividing them by 1x10^5 and renames them to \"buyside_price\" and \"sellside_price\"\n",
    "* and writes the result into a hive-partitioned set of parquet files in the directory \"minilob\"!\n",
    "\n",
    "Hints:\n",
    "\n",
    "In your select statement, you can rename and lightly manipulate columns as such:\n",
    "```\n",
    "SELECT \n",
    "  my_column * 2 AS twice_my_column\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a90cafd-9e75-4d3d-a16f-5a35455e4576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are a lot of ways to do this.  If we want something simple we can just use the relation\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "  time_stamp,\n",
    "  symbol,\n",
    "  buyside_price_00 / 10000 AS buyside_price,\n",
    "  sellside_price_00 / 10000 AS sellside_price\n",
    "FROM rel_a\n",
    "\"\"\"\n",
    "# test this query with a limited version of rel2\n",
    "transformed_relation = rel2.limit(10).query('rel_a', query)\n",
    "\n",
    "\n",
    "print(transformed_relation.pl().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ebb62-ae9d-4516-bf31-f276127dfcd7",
   "metadata": {},
   "source": [
    "Now that we know the relation works, let's go ahead and copy to our hive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea86eb8-ece6-42bc-a5a6-210eb0376d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_relation = rel2.query('rel_a', query)\n",
    "write_relation = transformed_relation.query(\"rel_b\", \"\"\"\n",
    "COPY (SELECT * FROM rel_b) \n",
    "TO minilob \n",
    "(FORMAT 'parquet', COMPRESSION 'zstd',\n",
    "PARTITION_BY (symbol))\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d706bd61-4ec6-4a24-9adc-3a3c23090e9a",
   "metadata": {},
   "source": [
    "Let's check our work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae169d-7d4e-4f84-b57c-61a2b520f09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn2=duckdb.connect(\":memory:\")\n",
    "conn2.sql(\"CREATE OR REPLACE TABLE lob AS SELECT * FROM read_parquet('minilob/**/*parquet', hive_partitioning=true)\")\n",
    "print(conn2.sql(\"SELECT COUNT(*) FROM lob\"))\n",
    "conn2.sql(\"SELECT * FROM lob\").limit(10).pl().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1870f143-855b-44e7-a282-00ef6bdaa387",
   "metadata": {},
   "source": [
    "This is obviously a pretty trivial \"ETL\" step, but a great deal more can be done, including relational joins with heterogenous data, aggregation, and more.  But to introduce aggregation in a different form, we'll look at Ray and Ray Data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
