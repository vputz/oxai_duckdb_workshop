{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a3bdf33-546d-4e12-9c99-8b41af5d46ff",
   "metadata": {},
   "source": [
    "# Transforms with Ray Data\n",
    "\n",
    "Ray Data (https://docs.ray.io/en/latest/data/data.html) is Ray's data service library for machine learning workloads.  Since this workshop is centered around data transformation, we're going to skip a lot of things that make it very intriguing, such as serving up data across clusters for Data Distributed Parallel training, substituting or augmenting data loaders for libraries like PyTorch, import from various sources like Huggingface, Iceberg, or more.\n",
    "\n",
    "We'll start simple, and build from our parquet hive we created back in section 3 (you'll need that to have been done for this section to work).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d915004-57b2-42d7-8f79-ceae7eab36c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from pathlib import Path\n",
    "\n",
    "# just in case /tmp writes to memory, let's use the current\n",
    "# directory to store spilled values\n",
    "# this version writes to $HOME/ray; make sure to delete it afterwards.\n",
    "# feel free to change this to whatever you want; the default\n",
    "# is usually /tmp/ray\n",
    "current_directory= Path.home().resolve() / \"ray\"\n",
    "# initialize ray... again.  Note--if you have not shut down the kernel from the previous notebook, it will still be running and this may connect\n",
    "# to it... which is fine here but use caution.  If you remove `address=\"auto\"` it will start a new cluster.\n",
    "ray.init(_temp_dir = str(current_directory), include_dashboard=True, ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f21ec-b57b-4b7d-877f-b1a66e1c4996",
   "metadata": {},
   "source": [
    "Ray data can read from various sources quite effectively, but generally can't tie them together like duckdb can.  Unfortunately even though ray data supports reading from SQL sources, a simple connection to duckdb tries to materialize queries all at once which can rapidly overload memory, and is not what we want here, so we'll read the hive files directly (which is supported).  I wouldn't be surprised to see a good interface for this soon.\n",
    "\n",
    "It should be noted that the Ray creators view Ray Data as \"last mile\" processing rather than a generic ETL engine (see https://discuss.ray.io/t/ray-is-not-meant-as-general-etl-tool/9826/11), and I think that was the original intent.  But spark can be tricky to set up and manage, and several libraries can be built or used on top of ray (Modin, Dask); even spark itself works under Ray (see the RayDP project, https://github.com/oap-project/raydp).  These are all worthwhile things to investigate; I wanted to look at Ray Data basics solely because it was tractable, used a minimal library set for a workshop, and provided a very different interface to DuckDB.  AWS recently used Ray Data to replace Spark in some tasks such as database compaction (https://aws.amazon.com/blogs/opensource/amazons-exabyte-scale-migration-from-apache-spark-to-ray-on-amazon-ec2/) to great effect.\n",
    "\n",
    "The Ray and Anyscale teams have also begun devoting more resources to ramping up their data engines, so I expect movement in this area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeadf2a-4c97-4ccc-89a5-97fbc312ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: ray data by default assumes hive partitioning if given a directory.  Different partitioning schema are available!\n",
    "base_dataset = ray.data.read_parquet(\"../3_etl_with_duckdb_part_2/minilob\")\n",
    "base_dataset.show(limit=1)\n",
    "dataset = base_dataset.limit(100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4fd667-d71a-42e5-9669-5d2b5a1dcc55",
   "metadata": {},
   "source": [
    "By default, ray data streams, storing objects in an object store, and represents in a dictionary format.  These are not the only options, of course; it can represent batches in arrow format, pandas format, or more, and we can leverage that.  In this case we'll use pandas because it's very standard and widespread, but conversion to arrow and polars can be extremely effective as well.\n",
    "\n",
    "Let's imagine a very simple ETL conversion:\n",
    "* Convert the nanosecond timestamp to a cyclical time of day\n",
    "* Calculate a midline price halfway between the buy and sell sides\n",
    "\n",
    "*Note* One reason Ray Data is unsuitable as a general ETL engine is that it is not very efficient at queries--not *remotely* as efficient as duckdb or any other database engine would be.  It has robust filtering capabilities, and robust transformations, but a Ray Data `Dataset` will iterate through all available objects in its data pool.  If you need that, it can be very effective--if not, other options are better!  In one case for data ingestion and ETL using one set of tools took seven hours and Ray Data did it in 30 minutes using a large cluster of cheap machines.  Tailor your strategy to your use case.\n",
    "\n",
    "In this case, we are limiting our dataset to the first 100,000 items for demonstration purposes.  That means we're not going to get all the data--not even all the symbols.  Let's see why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b57516-742e-42f9-9a0c-b8250686eac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's do a very simple aggregation to show that we only have one symbol in the first set of data\n",
    "# We'll cut and paste our little timing context from earlier\n",
    "import time\n",
    "\n",
    "# let's make a simple little profiler\n",
    "class Print_time:\n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.end = time.perf_counter()\n",
    "        self.elapsed = self.end - self.start\n",
    "        print(f\"Time taken: {self.elapsed:.4f} seconds\")\n",
    "\n",
    "def square(x: int)->int:\n",
    "    time.sleep(1)\n",
    "    return x*x\n",
    "\n",
    "with Print_time():\n",
    "    symbols = base_dataset.limit(10_000_000).aggregate(ray.data.aggregate.Unique(\"symbol\"))\n",
    "    print(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971a9b8c-0917-4c7c-b2bc-5da8cf1d9710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and compare to duckdb\n",
    "import duckdb\n",
    "\n",
    "with Print_time():\n",
    "    conn = duckdb.connect(\":memory:\")\n",
    "    rel=conn.sql(\"SELECT DISTINCT symbol FROM read_parquet(['../3_etl_with_duckdb_part_2/minilob/**/*parquet'], hive_partitioning=true)\")\n",
    "    print(rel.df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b45f83-58b1-47ef-be8d-33c7eceafcfc",
   "metadata": {},
   "source": [
    "There are some *very* interesting things going on there.  First note that Ray is much slower even on a tiny subset of data; in its element, *duckdb is incredible*.  Less obvious is that Ray Data operates in a less deterministic fashion and the order of processing is not deterministic (and the resulting dataset is not ordered unless you specifically `sort` it).\n",
    "\n",
    "On the surface, that makes Ray Data the underdog for most operations.  Let's do our ETL transformation anyway just for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1886d90-daeb-40fd-b1ea-f09e76d1ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Symbols = dict(NFLX=0, LSTR=1, SHLS=2, SOFI=3, CSCO=4, WING=5)\n",
    "\n",
    "def encode_timestamp(r: dict)->dict:\n",
    "    ns_per_day = 24 * 3600 * 10**9  # 86,400 seconds/day * 1e9 ns/s\n",
    "    ns_in_day = r['time_stamp'] % ns_per_day\n",
    "    fraction = ns_in_day / ns_per_day\n",
    "    return r | dict(ts_cos = np.cos(2 * np.pi * fraction), ts_sin = np.sin(2*np.pi * fraction))\n",
    "\n",
    "def add_midline(r: dict)->dict:\n",
    "    return r | dict(midline=r['buyside_price'] + r['sellside_price'] / 2)\n",
    "\n",
    "def encode_symbol(r: dict)->dict:\n",
    "    r['symbol'] = Symbols[r['symbol']]\n",
    "    return r\n",
    "\n",
    "with Print_time():\n",
    "    (dataset\n",
    "    .map(encode_timestamp)\n",
    "    .map(add_midline)\n",
    "    .map(encode_symbol)\n",
    "    ).show(limit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57646aa5-ef79-4d29-80d7-a774616ff226",
   "metadata": {},
   "source": [
    "If you're thinking \"that performance is lousy; surely the row-by-row conversion to dictionaries is incredibly slow\", well... you're right.  But it's useful to see how Ray Data deals with some forms of data in the background.  Luckily, there's a more efficient way to deal with data, and that is batching; Ray allows you to iterate over batches and transform them that way, and a batch itself can be transformed into various formats.  Let's look again at how to handle this sort of transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b7e05e-cd27-4c12-8212-76588cd3226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def transform_pandas_batch(df: pd.DataFrame)->pd.DataFrame:\n",
    "    ns_per_day = 24 * 3600* 10**9\n",
    "    ns_in_day = df['time_stamp'] % ns_per_day\n",
    "    fraction = ns_in_day / ns_per_day\n",
    "    \n",
    "    df['ts_cos'] = np.cos(2*np.pi*fraction)\n",
    "    df['ts_sin'] = np.sin(2*np.pi*fraction)\n",
    "    df['midline'] = (df['buyside_price']+df['sellside_price'])/2\n",
    "    df['symbol'] = df['symbol'].map(Symbols)\n",
    "    return df\n",
    "\n",
    "with Print_time():\n",
    "    (dataset\n",
    "    .map_batches(transform_pandas_batch, batch_format=\"pandas\")\n",
    "    ).show(limit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c886c7-c456-4328-9b55-cd9feacd90de",
   "metadata": {},
   "source": [
    "That's much much better.  Just for reference, let's try a third way.  Polars (http://pola.rs) is a relatively new kid on the block which leverages the arrow format and, written in rust, is extremely performant.  Let's see how it stacks up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ec25a-a45a-4aea-9872-5282c6eec08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pyarrow\n",
    "\n",
    "def transform_polars_batch(t: pyarrow.Table)->pyarrow.Table:\n",
    "    ns_per_day = 24 * 3600 * 10**9\n",
    "    df = pl.from_arrow(t)\n",
    "    result = (df\n",
    "          .with_columns(\n",
    "              ((pl.col(\"buyside_price\")+pl.col(\"sellside_price\"))/2).alias(\"midline\"),\n",
    "              np.cos(((pl.col(\"time_stamp\")%ns_per_day)/ns_per_day)).alias(\"ts_cos\"),\n",
    "              np.sin(((pl.col(\"time_stamp\")%ns_per_day)/ns_per_day)).alias(\"ts_csin\"),\n",
    "              pl.col(\"symbol\").map_elements(lambda x: Symbols.get(x, None), return_dtype=int).alias(\"symbol\")\n",
    "          ))\n",
    "    return result.to_arrow()\n",
    "\n",
    "with Print_time():\n",
    "    (dataset\n",
    "     .map_batches(transform_polars_batch, batch_format=\"pyarrow\")\n",
    "    ).show(limit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cd6ae0-48be-4270-8604-d1ce91b73db7",
   "metadata": {},
   "source": [
    "That difference in performance is interesting; sometimes at this scale it can seem longer, but generally it is shorter--and it can be quite significant, particularly as batch sizes increase and depending on the data conversino necessary.  Polars is an incredible dataframe library, superior to pandas in almost all respects; it behooves you to get familiar with it!\n",
    "\n",
    "One last thing: why is Ray.data described as a last-mile data transformation library?  Because it can present itself easily as a data loader in many machine learning frameworks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e8108-d123-4c2b-8c9f-fb46923d319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable = (\n",
    "    dataset\n",
    "    .map_batches(transform_polars_batch, batch_format=\"pyarrow\")\n",
    "    .iterator()\n",
    "    .iter_torch_batches(batch_size=10)\n",
    ")\n",
    "next(iter(iterable))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a8b733-67a9-4f6a-b273-2b67a0cb6551",
   "metadata": {},
   "source": [
    "At first blush, this doesn't look very interesting--but in the background, it has transparently mapped to torch tensors in batches.  Conversion to a torch dataloader is trivial, and when utilizing the ray training infrastructure, distribution of the dataset and appropriate sharding can be done in the background very elegantly.  This seems to be slightly in flux (Ray has recently removed the trivial `to_torch` conversion) but remains a useful way to shard datasets across a cluster for training or tuning, where `train.get_dataset_shard` can retrieve the \"correct\" shard of data for an individual node's (or process's) training data.\n",
    "\n",
    "In some ways, Ray Data as an ETL engine is a bit clunky; however, it scales *extremely* well horizontally across relatively cheap nodes, making it a very suitable engine for last-mile data transformations or transformations of a total data set.  However, its lack of SQL capabilities does limit it; for those tasks, seek solutions like Spark on Ray (RayDP) or ... in some rare cases... smallpond."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
