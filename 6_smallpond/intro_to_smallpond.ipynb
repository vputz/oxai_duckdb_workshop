{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03945b52-0240-4b9d-8045-cea17f427663",
   "metadata": {},
   "source": [
    "# The Bleeding Edge: SmallPond\n",
    "\n",
    "So if DuckDB excels at single-node processing, even up to terabytes of data, and Ray excels at multinode processing, particularly for the last mile, surely there's something to be gained by distributing duckdb processing across a cluster with Ray!\n",
    "\n",
    "That was probably the thought behind DeepSeek's recent (2 March) announcement of `smallpond` (https://github.com/deepseek-ai/smallpond).  Combined with their 3fs (\"Fire Flyer File System\") library, this is a distributed data processing system meant for terabytes to petabytes of data.\n",
    "\n",
    "And it all sounds great...\n",
    "... until you try to use it.\n",
    "\n",
    "As mentioned in blog posts, you probably don't need this and for most cases it's probably slower than duckdb or spark.  As http://definite.app/blog/smallpond notes,\n",
    "\n",
    "> Is smallpond for me?\n",
    "> tl;dr: probably not.\n",
    "> Whether you'd want to use smallpond depends on several factors:\n",
    "> Your Data Scale: If your dataset is under 10TB, smallpond adds unnecessary complexity and overhead. \n",
    "\n",
    "https://dataengineeringcentral.substack.com/p/smallpond-distributed-duckdb notes\n",
    "\n",
    "> It seems very early stage to me, reminds of a recent incubating Apache Project we looked at. Tons of work to do in the areas of â€¦\n",
    "> documentation\n",
    "> expanded functionality (read/write/transform)\n",
    "> increased usability\n",
    "> first class cloud integration (like s3)\n",
    "\n",
    "So.  There's basically *no* documentation.  It doesn't read directly from S3.  It doesn't read hive directly and easily (there's some related code in there but I couldn't get it doing what I asked for as it only wanted to partition on keys with base 10 integer values).\n",
    "\n",
    "So we're not going to do much with it besides a simple test and discussion.  In fact, because it doesn't directly handle hive, the first thing we're going to do is undo much of our good work by unhiving our previous hive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3896106-65d5-4cbc-8b55-07000a811bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "duckdb.sql(\"\"\"\n",
    "COPY \n",
    "(SELECT * FROM read_parquet(['../3_etl_with_duckdb_part_2/minilob/**/*parquet'], hive_partitioning=true))\n",
    "TO minilob.parquet \n",
    "(FORMAT 'parquet', COMPRESSION 'zstd')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae6f06b-fccd-45bd-ad4c-ce00f9986651",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    pl\n",
    "    .scan_parquet(\"minilob.parquet\")\n",
    "    .limit(10)\n",
    "    .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b017348c-390b-4dc8-8094-cbacfddd9dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# let's make a simple little profiler\n",
    "class Print_time:\n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.end = time.perf_counter()\n",
    "        self.elapsed = self.end - self.start\n",
    "        print(f\"Time taken: {self.elapsed:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60debe0-f57f-47a0-9b89-6c22fcda6843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smallpond\n",
    "from smallpond.logical.dataset import ParquetDataSet\n",
    "sp = smallpond.init(ray_address=\"auto\")\n",
    "\n",
    "df = sp.read_parquet(\"minilob.parquet\")\n",
    "\n",
    "# The magic of smallpond lies in partitioning, and what this effectively would do is partition this data between\n",
    "# all the values of SYMBOL so that eg CSCO would run on one node, LSTR would run on one node, etc.\n",
    "# however, this will likely fill up all available memory in the workshop because we are running on laptops!\n",
    "# df = df.repartition(6, hash_by=\"symbol\")\n",
    "\n",
    "# so instead, try running once, then repartitioning into two sets.\n",
    "df = df.repartition(6, by_rows=True)\n",
    "df = sp.partial_sql(\"SELECT symbol, min(buyside_price), max(buyside_price) FROM {0} GROUP BY symbol\", df, enable_temp_directory=True)\n",
    "\n",
    "# Show results\n",
    "with Print_time():\n",
    "    print(df.to_pandas().head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
