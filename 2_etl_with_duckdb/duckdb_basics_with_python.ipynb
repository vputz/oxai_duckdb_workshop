{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e659462a-3aea-45e3-9b60-2e4ae19f4d3e",
   "metadata": {},
   "source": [
    "# ETL With DuckDB\n",
    "\n",
    "At Quantum Signals we use ML models to do short and medium term prediction of markets.  One source of data we use is Limit Order Book data, which lists the ask and bid prices of a stock throughout the day.\n",
    "\n",
    "Each record of this data is pretty small, but the cardinality is extremely high and the timestamp is in nanoseconds--and even then, many events can have the same timestamp.  Let's use Python and DuckDB to query a subset of this information over S3, and do a quick ETL transform to write the midpoint price of each transaction to a hive-partitioned (by symbol) set of parquet files.\n",
    "\n",
    "# WARNING!\n",
    "\n",
    "For purposes of an easy workshop, credentials for this read-only S3 bucket are provided below.  They will be deleted after the workshop.  Never store credentials in a repository; it's a very bad way to go, but doing so here will greatly simplify getting our data and I thought it was worth it for the workshop itself.  This is a personal s3 bucket, so please don't abuse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95adda48-2466-4dc6-9bc9-978664b1c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "def create_connection():\n",
    "    # let's set up our duckdb in memory connection\n",
    "    conn=duckdb.connect(\":memory:\")\n",
    "    # the in-memory connection now must be extended to support https access to S3\n",
    "    conn.install_extension(\"httpfs\")\n",
    "    conn.load_extension(\"httpfs\")\n",
    "    # we'll also set up credentials.  Doing it this way is NOT recommended; never\n",
    "    # store secrets in repositories in production!\n",
    "    conn.execute(\"\"\"\n",
    "    CREATE SECRET secret (\n",
    "      TYPE S3,\n",
    "      KEY_ID 'DO801T8KVC4GP7XCU74A',\n",
    "      SECRET 'lZVY1vZlGUYJRim+f1WRpVYmv7PtJvYffheKSW4iJOQ',\n",
    "      REGION 'US',\n",
    "      ENDPOINT 'lon1.digitaloceanspaces.com'\n",
    "    )\"\"\")\n",
    "    return conn\n",
    "\n",
    "conn = create_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494349fd-7bb8-4227-a7ee-0e4df725cc23",
   "metadata": {},
   "source": [
    "A `DuckDBPyConnection` represents a connection to a database.  This can be tightly bound to queries, but also configured as necessary.  An *in-memory* duckdb connection stores all data in memory that isn't attached via outside storage, but makes a very effective tool for queries and transforms with external storage.\n",
    "\n",
    "A query itself is represented as a `DuckDBPyRelation`, and below we're going to query a compressed CSV file over S3.  On my home network, this took about 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc9bb6-01bb-4fc0-b0c8-67fd9888ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstr_rel = conn.sql(\"SELECT * FROM read_csv_auto('s3://thingotron-qs1/downloads/databento/6_stocks_2023/xnas-itch-20230101-20231230.mbp-10.LSTR.csv.zst')\")\n",
    "print(lstr_rel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2512f5ea-afe3-4a89-ad30-0ecc3349083a",
   "metadata": {},
   "source": [
    "So about 9 million rows, each with 74 columns.  30 seconds doesn't sound like much, but this was a low-traded stock; the compressed CSV was only about 356 Mb.  It's slow because CSV is a row-oriented format that is sloppy with its data formats and must be read entire to count rows or get summary information.\n",
    "\n",
    "Short form: CSV is terrible.  Never use CSV unless you cannot avoid it, and if you cannot avoid it, convert it to parquet or some other form when you can.  But while we have this relation, let's see what we can learn from it.\n",
    "\n",
    "A relation can be requeried, and in this manner you can chain relations and queries quite a way.  We can DESCRIBE the data to learn about what columns are there..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1091f6-949e-4101-95aa-28cfeeaaa02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstr_rel.query(\"lstr\", \"DESCRIBE lstr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6c0614-9d06-4b45-b1f6-5e027e37e590",
   "metadata": {},
   "source": [
    "The `query` on a relation takes a reference name as its first argument, so you can erfer to it in the following query...and you can chain queries together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7a9bc-0d1e-4923-9a00-9399c5aea831",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstr_rel.query(\"lstr1\", \"SELECT ts_event, ask_px_00, bid_px_00 FROM lstr1 LIMIT 10\").query(\"lstr2\", \"DESCRIBE lstr2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e9734-0a70-4985-b92d-16d45a7dec3c",
   "metadata": {},
   "source": [
    "Let's move on.  Luckily at least one year of this data, containing four stocks has been converted to parquet for us; let's look at CSCO in 2021 as above to compare, with a fresh new connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b0fb1-e4f6-43d6-aced-8348b8d8d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = create_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69adcf7-25df-426b-ae2c-19ae5a121a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "csco_rel = conn.sql(\"SELECT * FROM read_parquet(['s3://thingotron-qs1/artifacts/databento_1_CSCO_parquet:latest/CSCO.parquet'])\")\n",
    "print(csco_rel.shape)\n",
    "print(csco_rel.query(\"csco\", \"DESCRIBE csco\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c0a50-d89a-4668-934e-321694646ad5",
   "metadata": {},
   "source": [
    "This is a slightly different format (this is how we convert the raw databento data for our use at Quantum Signals), but I want to call out a couple of things:\n",
    "\n",
    "1. The speed of querying this parquet file is lightning fast compared to the compressed CSV, particularly since it has about 20 times the data.\n",
    "2. The size of this parquet file is only about 2.32 Gb compared to the previous CSV which was 356Mb\n",
    "\n",
    "For many, many, many cases parquet is a natural fit for file format.  It compresses well, allows columnar access so you can do complex queries without accessing the full dataset and materializing it in memory, and is widely supported by libraries such as Arrow (http://arrow.apache.org) and very efficient dataframe libraries like pandas and polars.\n",
    "\n",
    "Speaking of which, it is trivial to convert a duckdb SQL query into a polars or pandas dataframe.  We'll use polars here, but will avoid materializing the whole dataframe which could rapidly fill up memory and make it difficult to proceed; instead we'll take the first 1000 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039abcc8-3fba-4875-aba6-9d067cd60a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "polars_df = csco_rel.limit(1000).pl()\n",
    "pandas_df = csco_rel.limit(1000).df()\n",
    "print(polars_df.head())\n",
    "print(pandas_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5a8261-28bd-4097-8361-6f662640cce4",
   "metadata": {},
   "source": [
    "Lastly, let's look at memory usage of the pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a937f-9914-4225-9d7d-ec5cc709af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pandas_df.info(memory_usage=\"deep\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b90bdf-a858-48ea-9081-e3037fd82806",
   "metadata": {},
   "source": [
    "If the first 1000 rows of our query took 705*1024 bytes of data, and the full dataset had (194,025,498/1000=194,000) times that amount of data, we're looking at around 130Gb to fully materialize this query, and that's just one stock over one year.\n",
    "\n",
    "To transform this, we'll need to be clever; proceed to part 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
